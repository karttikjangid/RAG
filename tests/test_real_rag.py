"""
Real RAG Demo - Using actual RAG documentation PDF

This demonstrates the complete RAG pipeline using the 
'Retrieval Augmented Generation (RAG) for Everyone' PDF
"""

from pdf_ingestion import get_pdf_text
from chunking import get_chunks
from embedding import vector_embedding
from retrieval import search_best_chunks
from generation import generate_answer

def main():
    print("\n" + "=" * 70)
    print("üìö RAG SYSTEM - REAL DOCUMENT PROCESSING DEMO")
    print("=" * 70)
    
    # Step 1: Extract PDF text
    print("\n1Ô∏è‚É£  LOADING PDF DOCUMENT...")
    pdf_path = "../Retrieval Augmented Generation (RAG) for Everyone (1).pdf"
    
    text = get_pdf_text(pdf_path)
    
    if text.startswith("‚ùå"):
        print(f"Error: {text}")
        return
    
    print(f"\n‚úÖ Successfully loaded 57-page RAG document")
    print(f"   Characters: {len(text):,}")
    print(f"   Words: {len(text.split()):,}")
    
    # Step 2: Chunk the text
    print("\n2Ô∏è‚É£  CHUNKING TEXT...")
    chunks = get_chunks(text, chunk_size=300, overlap=75)
    print(f"   ‚úÖ Created {len(chunks)} chunks (300 chars each, 75 overlap)")
    
    # Step 3: Create embeddings
    print("\n3Ô∏è‚É£  CREATING VECTOR EMBEDDINGS...")
    vectors, model = vector_embedding(chunks)
    print(f"   ‚úÖ Generated {len(vectors)} vectors (384-dimensional)")
    
    # Step 4: Interactive Q&A
    print("\n" + "=" * 70)
    print("ü§ñ RAG SYSTEM READY - Ask questions about RAG!")
    print("=" * 70)
    
    # Predefined test questions
    test_questions = [
        "What is RAG?",
        "What are the components of RAG?",
        "What are the advantages of RAG?",
        "What is chunking in RAG?",
        "What is semantic caching?"
    ]
    
    print("\nTest Questions:")
    for i, q in enumerate(test_questions, 1):
        print(f"{i}. {q}")
    
    print("\nType a number (1-5) to ask a test question, or type your own question.")
    print("Type 'exit' to quit.\n")
    
    while True:
        user_input = input("‚ùì Your question: ").strip()
        
        if user_input.lower() == 'exit':
            print("\nüëã Goodbye!")
            break
        
        if not user_input:
            continue
        
        # Check if user selected a test question
        if user_input.isdigit() and 1 <= int(user_input) <= len(test_questions):
            query = test_questions[int(user_input) - 1]
            print(f"\nSelected: {query}")
        else:
            query = user_input
        
        print(f"\nüîç Searching knowledge base...")
        
        # Retrieve relevant chunks
        results = search_best_chunks(query, model, vectors, chunks, k=3)
        
        best_result = results[0]
        print(f"\nüìÑ Best Match (confidence: {best_result['score']:.4f}):")
        print(f"   {best_result['text'][:200]}...")
        
        # Generate answer
        print(f"\nü§ñ Generating answer...")
        try:
            answer = generate_answer(query, best_result['text'])
            print(f"\nüí¨ Answer:\n{answer}")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  LLM not available. Using retrieved context:")
            print(f"\n{best_result['text'][:500]}...")
        
        print("\n" + "-" * 70 + "\n")


if __name__ == "__main__":
    main()
